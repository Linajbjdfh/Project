# -*- coding: utf-8 -*-
"""HR_APP XGB.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16aYvgsTvYig0oP357If89AYNbQlfNinl

Structure

Importing Data csv

Cleaning data replace() function --> Filtering

Feature Engineering (preparing the dataset for ML)

selecting the features, scaling, creating dummies (Pandas is not good for keeping the mapping alive), binning --> reducing to main variables
Splitting and preparing for ML

Pick Model and train it

Evaluation

# Installing & Importing Data
"""

!pip install xgboost -U -q #u stands for upgrade #q stands for quiet/not so much output 
!pip install sklearn -U -q
!pip install shap -U -q

import pandas as pd
import numpy as np
import altair as alt
from xgboost import XGBRegressor
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import OneHotEncoder
import itertools
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
import shap
import pickle
from sklearn.linear_model import LogisticRegression, ElasticNet
from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier

data1 = pd.read_csv('/content/employee_survey_data.csv')

data2 = pd.read_csv('/content/general_data.csv')

data3 = pd.read_csv('/content/manager_survey_data.csv')



data1.info()

data2.info()

data3.info()

"""Merging the Data"""

data = data3.merge(data2, on="EmployeeID", how="right").merge(data1, on="EmployeeID", how="right") #merging the data

data['Attrition'] = data.Attrition.str.replace('No','0')
data['Attrition'] = data.Attrition.str.replace('Yes','1')

data['Attrition'] = data['Attrition'].astype('float')

data.head()

data.info()

data.shape

data.duplicated().sum() # to see if there are duplicated columns

data.isna().sum() #overvuew if there are 0/ Nans

data = data.dropna()

data.isna().sum()

data.describe()

data=data.drop(['MaritalStatus','StandardHours', 'EmployeeCount', 'Over18'], axis=1)

data.info()

data=data[['Attrition', 'BusinessTravel', 'Department','EducationField', 'Gender','JobRole', 'DistanceFromHome',
       'Education', 'EmployeeID', 'JobLevel',
        'MonthlyIncome', 'NumCompaniesWorked',
       'PercentSalaryHike', 'StockOptionLevel', 'TotalWorkingYears',
       'TrainingTimesLastYear', 'YearsAtCompany', 'YearsSinceLastPromotion',
       'YearsWithCurrManager', 'EnvironmentSatisfaction', 'JobSatisfaction',
       'WorkLifeBalance', 'JobInvolvement', 'PerformanceRating','Age']]

data.info()

"""add unique values here"""

#Count people who left and stayed at the company
data["Attrition"].value_counts()

"""# State of the Art HR """



alt.Chart(data).mark_circle(size=60).encode( #we need to add properties for streamlit #its hard to interpret
    x='MonthlyIncome',
    y='TotalWorkingYears',
    color='JobRole',
    tooltip=['JobRole', 'MonthlyIncome', 'TotalWorkingYears']
).interactive()

source = data #figure something out! 
alt.Chart(source).mark_bar().encode(
    x='MonthlyIncome',
    y='TotalWorkingYears',
    color='JobRole')

alt.Chart(data).mark_bar().encode( #we need something else for y
    x='JobRole',
    y='sum(EmployeeCount)',
    color='EducationField',
    tooltip=['EmployeeCount', 'EducationField']
)

#use value count for checking the outliers 
alt.Chart(data).mark_point().encode(
    alt.X('mean(Age):Q', scale=alt.Scale(zero=False)),
    y='JobRole:O',
    color='Gender:N',
    facet=alt.Facet('JobLevel:O', columns=2),
    tooltip=['Gender', 'JobRole']
).properties(
    width=200,
    height=100,
    )

alt.Chart(data).mark_rect().encode( #we would like to add more steps in JobSatisfaction
    x='JobLevel',
    y='JobRole',
    color='JobSatisfaction'
).properties(width=200)

alt.Chart(data).mark_bar().encode(x='Age',y='sum(Age)',color='Attrition').properties(width=700).interactive()

"""# Feature Engineering """

data.info()

"""NumCompaniesWork is a float --> integer"""

#selected_df = data[['Attrition','JobSatisfaction','YearsAtCompany','MonthlyIncome','Age','JobLevel','PerformanceRating','TrainingTimesLastYear']]

selected_df = data[['Attrition', 'JobRole','Gender','JobSatisfaction','YearsAtCompany',"NumCompaniesWorked"]]

X = selected_df.iloc[:,1:] #we select the X values from selected_df

Y = selected_df.Attrition #we select all rows and the column Attrition(index 0)

ohe_X = OneHotEncoder(sparse=False) #we are encoding values to save changes

X_ohe = ohe_X.fit_transform(X.iloc[:,0:2]) # we are not sure?

X.iloc[:,0:2]

data['JobRole'].value_counts()

X_ohe #lets check X_ohe

columns_X_ohe = list(itertools.chain(*ohe_X.categories_))

ohe_X.categories_

X_cat = pd.DataFrame(X_ohe, columns = columns_X_ohe)

X_cat

data['Department'].value_counts()

data['EducationField'].value_counts()

scaler = StandardScaler()

transformed_nummerical = scaler.fit_transform(X.iloc[:,2:])

X.iloc[:,2:] = transformed_nummerical

X.iloc[:,2:]

X.index = range(len(X))
X_cat.index = range(len(X_cat))

X_enc = X.iloc[:,2:].join(X_cat)

X_cat

"""# Splitting and Training """

X_train, X_test, Y_train, Y_test = train_test_split(X_enc, Y, test_size=0.2)

"""# Supervised ML"""

model_xgb = XGBRegressor()

model_lg = LogisticRegression()
model_el = ElasticNet()
model_rf = RandomForestRegressor(n_estimators=25)
model=RandomForestClassifier(n_estimators=10, criterion="entropy", random_state=21)

model_xgb.fit(X_train, Y_train)

model_lg.fit(X_train, Y_train)
model_el.fit(X_train, Y_train)
model_rf.fit(X_train, Y_train)
model.fit(X_train,Y_train)

print('Model XGB' + ' ' + str(model_xgb.score(X_train, Y_train)))

print('Model LG' + ' ' + str(model_lg.score(X_train, Y_train)))
print('Model EL' + ' ' + str(model_el.score(X_train, Y_train)))
print('Model RF' + ' ' + str(model_rf.score(X_train, Y_train)))
print('Model ' + ' ' + str(model.score(X_train, Y_train)))

"""# Evaluation for SML"""

Y_pred = model_xgb.predict(X_test)

mean_squared_error(Y_test, Y_pred, squared=False)

feat_importances = pd.Series(model_xgb.feature_importances_, index=X_enc.columns)
feat_importances.nlargest(20).plot(kind='barh')

X_enc

explainer = shap.TreeExplainer(model_xgb)

shap_values = explainer.shap_values(X_enc)

shap.summary_plot(shap_values, X_enc)

model_xgb.save_model('model_xgb.json')

pickle.dump(scaler, open('scaler.pkl','wb'))

pickle.dump(ohe_X, open('ohe.pkl','wb'))

X.to_json('X.json')
selected_df.to_json('selected_df.json')

pickle.dump(model_xgb, open('model.pkl','wb'))

pickle.dump(shap_values, open('shap_values.pkl','wb'))

"""# Unsupervised ML

# Pick Model and train it

# Evaluation
"""